{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: To build out the skeleton Pseudo Code for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For now__: Moving my notes from my one one one with Elizabeth and will build out more in the next few days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input .csv from experimental team to describe data\n",
    "2. CSV pulls the proper images from NDP (or wherever best storage location is determined to be)\n",
    "2a. There needs to be some integration with NDP to break the image into chunks with the appropriate zoom and then provide the original slice with locations of test and training images printed out.\n",
    "2b. I think to do this we are going to have to register the image to some kind of atlas... or somethign to determine regions so that the code can grab pictures\n",
    "3. Images need to be split\n",
    "3a. Regionally\n",
    "3b. Test Groups\n",
    "3c. Into Small enough sizes for most efficient processing by package\n",
    "3d. Magifications\n",
    "4. Threshold and segment\n",
    "4a. Ifthresholds (for general object identification) [July]\n",
    "4b. + Skeletonization (for specific highly branched morphometric analysis) [Robin]\n",
    "5. Split data into testing and training groups\n",
    "6. Shape factor analysis on both models (before ML model)\n",
    "7. Feed 4(1) and 4(2) separately to modified VAMPIRE package to build models\n",
    "8. Store created models\n",
    "9. Test models with testing data sets\n",
    "10. Data output\n",
    "11. Comprehensive CSV for optional additional analysis\n",
    "12. Data Visualization\n",
    "13. Save all data into some storage location (Google Drive?)\n",
    "\n",
    "\n",
    "Other Notes: \n",
    "1. Modified VAMPIRE package: Worth forking the Wirtz lab repository now and building on by ourselves possibly – adding that fork to our repository - COMPLETE\n",
    "2. Want a repository that works through Binder? Preferably so the Neonatalogy lab can do this straight through a web interface\n",
    "3. Along with outputs want to output the variables used for all of the steps on days that experiments were run with an easy print out maybe for lab notebook storage? (A way to integrate the electronic lab notebook I want to get more fine tuned for our lab with a standard lab notebook - and provide some under the hood knowledge for Neonatology)\n",
    "\n",
    "4. Add in an optimization step for what size of image is sufficient for cropping down the whole scan images \n",
    "5. Including an analysis before creating that analysis that selects the best # of shape modes and #coordinate points – maybe we could reach out the Denis Wirtz lab about this or build in our own integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Goal:\n",
    "\n",
    "1. Get a bunch of images from a slice of the ferret brain\n",
    "2. Put them in a folder\n",
    "3. Perform thresholding within the Jupyter Notebook\n",
    "3. Integrate vampire into the Jupyter Notebook\n",
    "4. Run those images and get an output within the notebook or specific folder (which should will need a results output)\n",
    "5. Save all of this information to some results folder\n",
    "6. See if it works in binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Goals:\n",
    "1. Build out ifthresholds more for immunohistochemistry stains\n",
    "2. Build in NDP regional registration and automatic image breakdown\n",
    "3. Integrate Google Drive\n",
    "4. New visualizations based on what came from paper\n",
    "5. Statistics possibly with Rthon\n",
    "6. Speed up any slow processes with Cython\n",
    "7. Integrate ifThresholds to pick and perform the best thresholding (or integrate a step to say whether this needs to be done or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Import necessary packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io\n",
    "from skimage import filters\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "from skimage import segmentation\n",
    "from skimage import color\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-white')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import image_slicer\n",
    "from scipy import ndimage as ndi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I mixed up the dapi and iba so down below it doesn't make sense the final c1 are iba and the final c2s are dapi and I'll fix documentation later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: User Inputs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data folder to the stain you are running morphology on (right now only works one stain at a time)\n",
    "#rerun for each individual stain\n",
    "\n",
    "#Folder breakdown: stain>animal>region>actual images\n",
    "data_folder = '/Users/hhelmbre/Desktop/KH_LPS_ferret'\n",
    "\n",
    "#Image type of your images (they should all be the same type)\n",
    "image_file_type = '.tif'\n",
    "file_type_new = '.png'\n",
    "\n",
    "#Enter the name of your stain as a string\n",
    "stain1 = 'Iba1' #c1\n",
    "stain2 = 'dapi' #c2\n",
    "\n",
    "experiment_name = 'practice'\n",
    "\n",
    "region_list = ['corpus_callosum', 'hippocampus', 'cortex', 'white_matter', 'basal_ganglia', 'thalamus']\n",
    "treatment_list = ['NT', 'AcAc', 'OGD1h', 'OGD2h', 'Epo']\n",
    "female_list = ['68-24', '68-28', '68-35', '68-36', '68-53', '68-60', '44-48', '46-45', '46-47', '64-19', '68-37', '68-43', '68-49', '68-20']\n",
    "male_list = ['-68-6_', '68-6b', '68-11', '68-12', '68-18', '68-68', '44-42', '46-39', '46-41', '60-1', '64-1', '64-43']\n",
    "\n",
    "\n",
    "NT_list = ['68-6_', '68-6b', '68-24', '46-39', '46-45', '60-1', '68-20']\n",
    "AcAc_list = ['68-49', '64-43', '68-53', '68-35', '68-11']\n",
    "OGD1h_list = ['68-68', '46-41', '46-47', '68-37', '68-28']\n",
    "OGD2h_list = ['68-36', '44-42', '44-48', '64-1', '64-19', '68-12']\n",
    "Epo_list = ['68-18', '68-60', '64-7', '68-43']\n",
    "\n",
    "\n",
    "random_state_num = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Threshold and segment everything and then split test and train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir(data_folder)\n",
    "folder_list = np.asarray(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for folders in folder_list:\n",
    "    if mac_annoyance in str(folders):\n",
    "        folder_list = np.delete(folder_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Split up the stains and put them into their own folders*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1b: Split the images into their individual stains*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "for non_channel_split_images in folder_list:\n",
    "    name = str(data_folder + '/' + non_channel_split_images)\n",
    "    im = io.imread(name)\n",
    "    channel1 = im[0, :, :]\n",
    "    channel2= im[1, :, :]\n",
    "    filename = non_channel_split_images.replace(image_file_type, \"\")\n",
    "    channel1 = Image.fromarray(np.uint16(channel1))\n",
    "    channel1.save(str(data_folder + '/' + filename + '_' + stain1 + file_type_new))\n",
    "    channel2 = Image.fromarray(np.uint16(channel2))\n",
    "    channel2.save(str(data_folder + '/' + filename + '_' + stain2 + file_type_new))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1c: Split the images into 4ths*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(data_folder)\n",
    "file_list = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for images in file_list:\n",
    "    if '.tif' in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_number = 4\n",
    "for files in file_list:\n",
    "    image_slicer.slice(str(data_folder + '/' + files), slice_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(data_folder)\n",
    "file_list = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Move split images into their proper stain folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(str(data_folder + '/' + stain1))\n",
    "os.mkdir(str(data_folder + '/' + stain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tiled_images in file_list:\n",
    "    conditional = str(str(tiled_images)[-6].isdigit())\n",
    "    if conditional == 'True':\n",
    "        if stain1 in tiled_images:\n",
    "            shutil.move(str(data_folder + '/' + tiled_images), str(data_folder + '/' + stain1 + '/' + tiled_images))\n",
    "        elif stain2 in tiled_images:\n",
    "            shutil.move(str(data_folder + '/' + tiled_images), str(data_folder + '/' + stain2 + '/' + tiled_images))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split into the proper sexes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(str(data_folder + '/' + stain1))\n",
    "file_list_stain1 = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list_stain1:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(str(data_folder + '/' + stain1 + '/' + 'female'))\n",
    "os.mkdir(str(data_folder + '/' + stain1 + '/' + 'male'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nonsexed_images in file_list_stain1:\n",
    "    new_data_folder = data_folder + '/' + stain1 + '/'\n",
    "    for animal_codes in female_list:\n",
    "        if animal_codes in nonsexed_images:\n",
    "            shutil.move(str(new_data_folder + nonsexed_images), str(new_data_folder + 'female/' + nonsexed_images))\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(str(data_folder + '/' + stain1))\n",
    "file_list_stain1 = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list_stain1:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list_stain1 = np.delete(file_list_stain1, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for images in file_list_stain1:\n",
    "    if '.png' in images:\n",
    "        k+=1\n",
    "    else:\n",
    "        file_list_stain1 = np.delete(file_list_stain1, (k), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nonsexed_images in file_list_stain1:\n",
    "    new_data_folder = str(data_folder + '/' + stain1 + '/')\n",
    "    for animal_codes in male_list:\n",
    "        if animal_codes in nonsexed_images:\n",
    "            shutil.move(str(new_data_folder + nonsexed_images), str(new_data_folder + 'male/' + nonsexed_images))            \n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split into treatment groups*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "sexes = ['female', 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sex in sexes:\n",
    "    new_data_folder = data_folder + '/' + stain1 + '/' + sex + '/'\n",
    "    arr = os.listdir(new_data_folder)\n",
    "    file_list = np.asarray(arr)\n",
    "    \n",
    "    k=0\n",
    "    mac_annoyance= 'DS_Store'\n",
    "    for images in file_list:\n",
    "        if mac_annoyance in str(images):\n",
    "            file_list = np.delete(file_list, (k), axis=0)\n",
    "        else:\n",
    "            k+=1\n",
    "    for treatments in treatment_list:\n",
    "        os.mkdir(str(new_data_folder + treatments))\n",
    "        \n",
    "    \n",
    "    for sex_split_images in file_list:\n",
    "        for animal_codes in NT_list:\n",
    "            if animal_codes in sex_split_images:\n",
    "                shutil.move(str(new_data_folder + sex_split_images), str(new_data_folder + treatment_list[0] + '/' + sex_split_images))\n",
    "            else:\n",
    "                pass\n",
    "        for animal_codes in AcAc_list:\n",
    "            if animal_codes in sex_split_images:\n",
    "                shutil.move(str(new_data_folder + sex_split_images), str(new_data_folder + treatment_list[1] + '/' + sex_split_images))\n",
    "            else:\n",
    "                pass\n",
    "        for animal_codes in OGD1h_list:\n",
    "            if animal_codes in sex_split_images:\n",
    "                shutil.move(str(new_data_folder + sex_split_images), str(new_data_folder + treatment_list[2] + '/' + sex_split_images))\n",
    "            else:\n",
    "                pass   \n",
    "        for animal_codes in OGD2h_list:\n",
    "            if animal_codes in sex_split_images:\n",
    "                shutil.move(str(new_data_folder + sex_split_images), str(new_data_folder + treatment_list[3] + '/' + sex_split_images))\n",
    "            else:\n",
    "                pass \n",
    "        for animal_codes in Epo_list:\n",
    "            if animal_codes in sex_split_images:\n",
    "                shutil.move(str(new_data_folder + sex_split_images), str(new_data_folder + treatment_list[4] + '/' + sex_split_images))\n",
    "            else:\n",
    "                pass "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split Regions within each folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sex in sexes:\n",
    "    for treatments in treatment_list:\n",
    "        new_data_folder = data_folder + '/' + stain1 + '/' + sex + '/' + treatments + '/'\n",
    "        arr = os.listdir(str(new_data_folder))\n",
    "        file_list = np.asarray(arr)\n",
    "        k=0\n",
    "        mac_annoyance= 'DS_Store'\n",
    "        for images in file_list:\n",
    "            if mac_annoyance in str(images):\n",
    "                file_list = np.delete(file_list, (k), axis=0)\n",
    "            else:\n",
    "                k+=1\n",
    "        for regions in region_list:\n",
    "            os.mkdir(str(new_data_folder + regions))\n",
    "        for stain1_images in file_list:\n",
    "            region_code = str(stain1_images[12])\n",
    "            if region_code == '1':\n",
    "                shutil.move(str(new_data_folder + stain1_images), str(new_data_folder + region_list[0] + '/' + stain1_images))\n",
    "            elif region_code == '2':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[1] + '/' + stain1_images))\n",
    "            elif region_code == '3':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[2] + '/' + stain1_images))\n",
    "            elif region_code == '5':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[3] + '/' + stain1_images))\n",
    "            elif region_code == '6':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[4] + '/' + stain1_images))\n",
    "            elif region_code == '7':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[5] + '/' + stain1_images))\n",
    "            else:\n",
    "                pass\n",
    "        for stain1_images in file_list:\n",
    "            region_code = str(stain1_images[11])\n",
    "            if region_code == '1':\n",
    "                shutil.move(str(new_data_folder + stain1_images), str(new_data_folder + region_list[0] + '/' + stain1_images))\n",
    "            elif region_code == '2':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[1] + '/' + stain1_images))\n",
    "            elif region_code == '3':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[2] + '/' + stain1_images))\n",
    "            elif region_code == '5':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[3] + '/' + stain1_images))\n",
    "            elif region_code == '6':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[4] + '/' + stain1_images))\n",
    "            elif region_code == '7':\n",
    "                shutil.move(str(new_data_folder + stain1_images),str(new_data_folder + region_list[5] + '/' + stain1_images))\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Assign test and train groups for each sex>treatment>region group*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_split_paths = []\n",
    "for sex in sexes:\n",
    "    for treatments in treatment_list:\n",
    "        for regions in region_list:\n",
    "            path = data_folder + '/' + stain1 + '/' + sex + '/' + treatments + '/' + regions\n",
    "            test_split_paths.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(str(data_folder + '/' + 'train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in test_split_paths:\n",
    "    arr = os.listdir(str(paths))\n",
    "    file_list_train = np.asarray(arr)\n",
    "    k=0\n",
    "    mac_annoyance= 'DS_Store'\n",
    "    for pathways in file_list_train:\n",
    "        if mac_annoyance in str(images):\n",
    "            file_list_train = np.delete(file_list_train, (k), axis=0)\n",
    "        else:\n",
    "            k+=1\n",
    "            \n",
    "    X_train, X_test= train_test_split(file_list_train, test_size=0.25, random_state=random_state_num)\n",
    "    \n",
    "    for names in file_list_train:\n",
    "        if names in X_train[:]:\n",
    "            shutil.move(str(paths + '/' + names), str(data_folder + '/' + 'train/' + names))\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Grab the DAPI Images for each set and then rename the images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_train1 = os.listdir(str(data_folder + '/' + 'train'))\n",
    "file_list_train1 = np.asarray(arr_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "mac_annoyance= 'DS_Store'\n",
    "for files in file_list_train1:\n",
    "    if mac_annoyance in str(files):\n",
    "        file_list_train1 = np.delete(file_list_train1, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr_stain2 = os.listdir(str(data_folder + '/' + stain2))\n",
    "file_list_stain2 = np.asarray(arr_stain2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in file_list_stain2:\n",
    "    if mac_annoyance in str(files):\n",
    "        file_list_stain2 = np.delete(file_list_stain2, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_number= 1\n",
    "for names in file_list_train1:\n",
    "    iba_name = str(names)\n",
    "\n",
    "    if im_number < 10:\n",
    "        os.rename(str(data_folder + '/' + 'train/' + names), str(data_folder + '/' + 'train/' + 'xy' + '0' + str(im_number) + 'c2.png'))\n",
    "        \n",
    "    else:\n",
    "        os.rename(str(data_folder + '/' + 'train/' + names), str(data_folder + '/' + 'train/' + 'xy' + str(im_number) + 'c2.png'))\n",
    "    \n",
    "    dapi_name = iba_name.replace(stain1, stain2)\n",
    "    \n",
    "    if im_number < 10:\n",
    "        os.rename(str(data_folder + '/' + stain2 + '/' + dapi_name), str(data_folder + '/' + 'train/' + 'xy' + '0' + str(im_number) + 'c1.png'))\n",
    "        \n",
    "    else:\n",
    "        os.rename(str(data_folder + '/' + stain2 + '/' + dapi_name), str(data_folder + '/' + 'train/' + 'xy' + str(im_number) + 'c1.png'))\n",
    "        \n",
    "    \n",
    "    im_number +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Grab all the dapi images for the test groups*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in test_split_paths:\n",
    "    test_stain1 = os.listdir(str(paths))\n",
    "    test_stain1 = np.asarray(test_stain1)\n",
    "    for files in test_stain1:\n",
    "        if mac_annoyance in str(files):\n",
    "            test_stain1 = np.delete(test_stain1, (k), axis=0)\n",
    "        else:\n",
    "            k+=1\n",
    "    \n",
    "    im_number= 1\n",
    "    for names in test_stain1:\n",
    "        iba_name = str(names)\n",
    "\n",
    "        if im_number < 10:\n",
    "            os.rename(str(paths + '/' + names), str(paths + '/' + 'xy' + '0' + str(im_number) + 'c2.png'))\n",
    "\n",
    "        else:\n",
    "            os.rename(str(paths + '/' + names), str(paths + '/' + 'train/' + 'xy' + str(im_number) + 'c2.png'))\n",
    "\n",
    "        dapi_name = iba_name.replace(stain1, stain2)\n",
    "\n",
    "        if im_number < 10:\n",
    "            os.rename(str(data_folder + '/' + stain2 + '/' + dapi_name), str(paths + '/' + 'xy' + '0' + str(im_number) + 'c1.png'))\n",
    "\n",
    "        else:\n",
    "            os.rename(str(data_folder + '/' + stain2 + '/' + dapi_name), str(paths + '/' + 'train/' + 'xy' + str(im_number) + 'c1.png'))\n",
    "\n",
    "\n",
    "        im_number +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3: Threshold and Segment Images*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Thresholding the training group*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list = os.listdir(str(data_folder + '/' + 'train'))\n",
    "train_list = np.asarray(train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "for files in train_list:\n",
    "    if mac_annoyance in str(files):\n",
    "        train_list = np.delete(train_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thresholding on the training files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/ipykernel_launcher.py:11: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "os.mkdir(str(data_folder + '/' + 'train/threshold_ims'))\n",
    "for images in train_list:\n",
    "\n",
    "    #Creating a folder for the new thresholded images\n",
    "    #Going through each of the images to get their binarized images and measurement info\n",
    "    name = str(data_folder + '/' + 'train/' + images)\n",
    "    im = io.imread(name)\n",
    "    threshold = filters.threshold_otsu(im)\n",
    "    binary = morphology.closing(im > threshold, morphology.square(1))\n",
    "    label_image = measure.label(binary, return_num=False, connectivity=2)\n",
    "    binary2 = morphology.remove_small_objects(label_image, min_size=25, connectivity=2, in_place=True)\n",
    "    fill_cells = ndi.binary_fill_holes(binary2)\n",
    "\n",
    "    # Saving the thresheld images to their own folder with modified names\n",
    "    im_to_save = Image.fromarray(fill_cells)\n",
    "    new_name = str(data_folder + '/train/' + 'threshold_ims' + '/' + images)\n",
    "    new_name = new_name.replace('.png','_threshold.png')\n",
    "    im_to_save.save(new_name)                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Thresholding on the Testing Files*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/ipykernel_launcher.py:22: UserWarning: Only one label was provided to `remove_small_objects`. Did you mean to use a boolean array?\n"
     ]
    }
   ],
   "source": [
    "for paths in test_split_paths:\n",
    "    \n",
    "    k = 0\n",
    "    test_stain1 = os.listdir(str(paths))\n",
    "    test_stain1 = np.asarray(test_stain1)\n",
    "    for files in test_stain1:\n",
    "        if mac_annoyance in str(files):\n",
    "            test_stain1 = np.delete(test_stain1, (k), axis=0)\n",
    "        else:\n",
    "            k+=1\n",
    "    os.mkdir(str(paths + '/' + 'threshold_ims'))\n",
    "            \n",
    "    for images in test_stain1:\n",
    "\n",
    "        #Creating a folder for the new thresholded images\n",
    "        #Going through each of the images to get their binarized images and measurement info\n",
    "        name = str(paths + '/' + images)\n",
    "        im = io.imread(name)\n",
    "        threshold = filters.threshold_otsu(im)\n",
    "        binary = morphology.closing(im > threshold, morphology.square(1))\n",
    "        label_image = measure.label(binary, return_num=False, connectivity=2)\n",
    "        binary2 = morphology.remove_small_objects(label_image, min_size=25, connectivity=2, in_place=True)\n",
    "        fill_cells = ndi.binary_fill_holes(binary2)\n",
    "\n",
    "        # Saving the thresheld images to their own folder with modified names\n",
    "        im_to_save = Image.fromarray(fill_cells)\n",
    "        new_name = str(paths + '/threshold_ims' + '/' + images)\n",
    "        new_name = new_name.replace('.png','_threshold.png')\n",
    "        im_to_save.save(new_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through all the threshold paths + the threshold folder and assign 8 of them for training and 2 of them for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vampire Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following will pull up the GUI as a popup - I want to be able to input values into the GUI straight from here not have to point and click*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We are going to work with what we have an have a print out here of what to input in the VAMPIRE GUI - it is still faster than it used to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Build and then apply model with the vampire GUI (will work on making it not GUI later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Create the CSVs for Building and applying the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model repository to store the output information\n",
    "#create a csv for building the model\n",
    "#create a csv for applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the directory in your data folder to put all information related to the model\n",
    "os.mkdir(str(data_folder + '/' + 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the csv for building a model\n",
    "data = [['1', stain, str(data_folder + '/' + 'train'), 'ch1', 'iba1']]\n",
    "build_model_csv = pd.DataFrame(data, columns = ['set ID', 'condition', 'set location', 'tag', 'note']) \n",
    "\n",
    "#saves csv to newly created model directory\n",
    "build_model_csv.to_csv(data_folder + '/' + 'model/' + 'images_to_build_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the csv for applying a model\n",
    "apply_model_csv = pd.DataFrame(columns = ['set ID', 'condition', 'set location', 'tag', 'note'])\n",
    "\n",
    "set_number = 1\n",
    "for folders in test_paths:\n",
    "    str_set_number = str(set_number)\n",
    "    pattern = str(data_folder + '/' + '(.*?)' + '/threshold_applied_images/')\n",
    "    substring = re.search(pattern, folders).group(1)\n",
    "\n",
    "    df2 = pd.DataFrame({'set ID': [set_number], 'condition': [stain], 'set location': [folders], 'tag': ['ch1'], 'note': ['iba1']})\n",
    "    apply_model_csv = apply_model_csv.append(df2)\n",
    "    \n",
    "    set_number += 1\n",
    "    \n",
    "apply_model_csv.to_csv(data_folder + '/' + 'model/' + 'images_to_apply_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step X: Print the inputs that should be added to VAMPIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Model CSV Path: /Users/hhelmbre/Desktop/6-17-2020-fiberfimages/iba1/model/images_to_build_model.csv\n",
      "Number of Shape Models (Recommended): 5\n",
      "Number of Shape Coordinates (Recommended): 50\n",
      "Model Name: practice\n",
      "Model to Apply: /Users/hhelmbre/Desktop/6-17-2020-fiberfimages/iba1/model/practice\n"
     ]
    }
   ],
   "source": [
    "print('Build Model CSV Path:', str(data_folder + '/' + 'model/' + 'images_to_build_model.csv'))\n",
    "\n",
    "print('Number of Shape Models (Recommended):', '5')\n",
    "print('Number of Shape Coordinates (Recommended):', '50')\n",
    "print('Model Name:', experiment_name)\n",
    "\n",
    "print('Apply Model CSV Path:', str(data_folder + '/' + 'model/' + 'images_to_apply_model.csv'))\n",
    "print('Model to Apply:', str(data_folder + '/' + 'model/' + experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Open the VAMPIRE GUI, build, and then apply the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vampireanalysis\n",
    "from vampireanalysis import vampire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## getboundary.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 2897, in get_loc\n",
      "    return self._engine.get_loc(key)\n",
      "  File \"pandas/_libs/index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'ch1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/vampireanalysis/vampire.py\", line 124, in <lambda>\n",
      "    b2 = Button(rows[5], text='build model', width=12, command=(lambda e=ents: Model(e, True, progress_bar)))\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/vampireanalysis/vampire.py\", line 83, in Model\n",
      "    getboundary(csv, progress_bar, entries)  # create registry csv and boundary stack\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/vampireanalysis/getboundary.py\", line 51, in getboundary\n",
      "    ch1 = ui['ch1'][setfolderidx]\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/pandas/core/frame.py\", line 2995, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 2899, in get_loc\n",
      "    return self._engine.get_loc(self._maybe_cast_indexer(key))\n",
      "  File \"pandas/_libs/index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'ch1'\n"
     ]
    }
   ],
   "source": [
    "vampire()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 8: New visualization of VAMPIRE data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This should probably be built into VAMPIRE and not just into this notebook - think about this for the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still needs to be visualization steps here because what is output by vampire just is not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
