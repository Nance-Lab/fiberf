{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose: To build out the skeleton Pseudo Code for this project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For now__: Moving my notes from my one one one with Elizabeth and will build out more in the next few days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input .csv from experimental team to describe data\n",
    "2. CSV pulls the proper images from NDP (or wherever best storage location is determined to be)\n",
    "2a. There needs to be some integration with NDP to break the image into chunks with the appropriate zoom and then provide the original slice with locations of test and training images printed out.\n",
    "2b. I think to do this we are going to have to register the image to some kind of atlas... or somethign to determine regions so that the code can grab pictures\n",
    "3. Images need to be split\n",
    "3a. Regionally\n",
    "3b. Test Groups\n",
    "3c. Into Small enough sizes for most efficient processing by package\n",
    "3d. Magifications\n",
    "4. Threshold and segment\n",
    "4a. Ifthresholds (for general object identification) [July]\n",
    "4b. + Skeletonization (for specific highly branched morphometric analysis) [Robin]\n",
    "5. Split data into testing and training groups\n",
    "6. Shape factor analysis on both models (before ML model)\n",
    "7. Feed 4(1) and 4(2) separately to modified VAMPIRE package to build models\n",
    "8. Store created models\n",
    "9. Test models with testing data sets\n",
    "10. Data output\n",
    "11. Comprehensive CSV for optional additional analysis\n",
    "12. Data Visualization\n",
    "13. Save all data into some storage location (Google Drive?)\n",
    "\n",
    "\n",
    "Other Notes: \n",
    "1. Modified VAMPIRE package: Worth forking the Wirtz lab repository now and building on by ourselves possibly – adding that fork to our repository - COMPLETE\n",
    "2. Want a repository that works through Binder? Preferably so the Neonatalogy lab can do this straight through a web interface\n",
    "3. Along with outputs want to output the variables used for all of the steps on days that experiments were run with an easy print out maybe for lab notebook storage? (A way to integrate the electronic lab notebook I want to get more fine tuned for our lab with a standard lab notebook - and provide some under the hood knowledge for Neonatology)\n",
    "\n",
    "4. Add in an optimization step for what size of image is sufficient for cropping down the whole scan images \n",
    "5. Including an analysis before creating that analysis that selects the best # of shape modes and #coordinate points – maybe we could reach out the Denis Wirtz lab about this or build in our own integration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current Goal:\n",
    "\n",
    "1. Get a bunch of images from a slice of the ferret brain\n",
    "2. Put them in a folder\n",
    "3. Perform thresholding within the Jupyter Notebook\n",
    "3. Integrate vampire into the Jupyter Notebook\n",
    "4. Run those images and get an output within the notebook or specific folder (which should will need a results output)\n",
    "5. Save all of this information to some results folder\n",
    "6. See if it works in binder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Goals:\n",
    "1. Build out ifthresholds more for immunohistochemistry stains\n",
    "2. Build in NDP regional registration and automatic image breakdown\n",
    "3. Integrate Google Drive\n",
    "4. New visualizations based on what came from paper\n",
    "5. Statistics possibly with Rthon\n",
    "6. Speed up any slow processes with Cython\n",
    "7. Integrate ifThresholds to pick and perform the best thresholding (or integrate a step to say whether this needs to be done or not)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Import necessary packages*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io\n",
    "from skimage import filters\n",
    "from skimage import measure\n",
    "from skimage import morphology\n",
    "from skimage import segmentation\n",
    "from skimage import color\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#plt.style.use('seaborn-white')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import image_slicer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: User Inputs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data folder to the stain you are running morphology on (right now only works one stain at a time)\n",
    "#rerun for each individual stain\n",
    "\n",
    "#Folder breakdown: stain>animal>region>actual images\n",
    "data_folder = '/Users/hhelmbre/Desktop/KH_LPS_ferret'\n",
    "\n",
    "#Image type of your images (they should all be the same type)\n",
    "image_file_type = '.tif'\n",
    "file_type_new = '.png'\n",
    "\n",
    "#Enter the name of your stain as a string\n",
    "stain1 = 'Iba1' #c1\n",
    "stain2 = 'dapi' #c2\n",
    "\n",
    "experiment_name = 'practice'\n",
    "\n",
    "region_list = ['corpus_callosum', 'hippocampus', 'cortex', 'white_matter', 'basal_ganglia', 'thalamus']\n",
    "treatment_list = ['NT', 'AcAc', 'OGD2h', 'Epo', 'OGD1h']\n",
    "female_list = ['68-24', '68-28', '68-35', '68-36', '68-53', '68-60', '44-48', '46-45', '46-47', '64-19', '68-37', '68-43', '68-49', '68-20']\n",
    "male_list = ['68-6_', '68-6b', '68-11', '68-12', '68-18', '68-68', '44-42', '46-39', '46-41', '60-1', '64-1', '64-43']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Threshold and segment everything and then split test and train*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_list = os.listdir(data_folder)\n",
    "folder_list = np.asarray(folder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for folders in folder_list:\n",
    "    if mac_annoyance in str(folders):\n",
    "        folder_list = np.delete(folder_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Split up the stains and put them into their own folders*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1b: Split the images into their individual stains*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for non_channel_split_images in folder_list:\n",
    "    name = str(data_folder + '/' + non_channel_split_images)\n",
    "    im = io.imread(name)\n",
    "    channel1 = im[0, :, :]\n",
    "    channel2= im[1, :, :]\n",
    "    filename = non_channel_split_images.replace(image_file_type, \"\")\n",
    "    channel1 = Image.fromarray(np.uint16(channel1))\n",
    "    channel1.save(str(data_folder + '/' + filename + '_' + stain1 + file_type_new))\n",
    "    channel2 = Image.fromarray(np.uint16(channel2))\n",
    "    channel2.save(str(data_folder + '/' + filename + '_' + stain2 + file_type_new))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1c: Split the images into 4ths*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(data_folder)\n",
    "file_list = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=0\n",
    "for images in file_list:\n",
    "    if '.tif' in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_number = 4\n",
    "for files in file_list:\n",
    "    image_slicer.slice(str(data_folder + '/' + files), slice_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(data_folder)\n",
    "file_list = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Move split images into their proper stain folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(str(data_folder + '/' + stain1))\n",
    "os.mkdir(str(data_folder + '/' + stain2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tiled_images in file_list:\n",
    "    conditional = str(str(tiled_images)[-6].isdigit())\n",
    "    if conditional == 'True':\n",
    "        if stain1 in tiled_images:\n",
    "            shutil.move(str(data_folder + '/' + tiled_images), str(data_folder + '/' + stain1 + '/' + tiled_images))\n",
    "        elif stain2 in tiled_images:\n",
    "            shutil.move(str(data_folder + '/' + tiled_images), str(data_folder + '/' + stain2 + '/' + tiled_images))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split into the proper sexes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(str(data_folder + '/' + stain1))\n",
    "file_list_stain1 = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list_stain1:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(str(data_folder + '/' + stain1 + '/' + 'female'))\n",
    "os.mkdir(str(data_folder + '/' + stain1 + '/' + 'male'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nonsexed_images in file_list_stain1:\n",
    "    new_data_folder = data_folder + '/' + stain1 + '/'\n",
    "    for animal_codes in female_list:\n",
    "        if animal_codes in nonsexed_images:\n",
    "            shutil.move(str(new_data_folder + nonsexed_images), str(new_data_folder + 'female/' + nonsexed_images))\n",
    "        else:\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(str(data_folder + '/' + stain1))\n",
    "file_list_stain1 = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list_stain1:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list_stain1 = np.delete(file_list_stain1, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "for images in file_list_stain1:\n",
    "    if '.png' in images:\n",
    "        k+=1\n",
    "    else:\n",
    "        file_list_stain1 = np.delete(file_list_stain1, (k), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "for nonsexed_images in file_list_stain1:\n",
    "    new_data_folder = data_folder + '/' + stain1 + '/'\n",
    "    for animal_codes in male_list:\n",
    "        if animal_codes in nonsexed_images:\n",
    "            shutil.move(str(new_data_folder + nonsexed_images), str(new_data_folder + 'male/' + nonsexed_images))            \n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split into treatment groups*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(str(data_folder + '/' + stain1))\n",
    "file_list_stain1 = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list_stain1:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatments in treatment_list:\n",
    "    os.mkdir(str(data_folder + '/' + stain1 + '/' + regions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Split Regions within each folder*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir(str(data_folder + '/' + stain1))\n",
    "file_list_stain1 = np.asarray(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for macs to get rid of an aesthetic file called '.DS_Store'\n",
    "k=0\n",
    "mac_annoyance= 'DS_Store'\n",
    "for images in file_list_stain1:\n",
    "    if mac_annoyance in str(images):\n",
    "        file_list = np.delete(file_list, (k), axis=0)\n",
    "    else:\n",
    "        k+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for regions in region_list:\n",
    "    os.mkdir(str(data_folder + '/' + stain1 + '/' + regions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stain1_images in file_list_stain1:\n",
    "    region_code = str(stain1_images[12])\n",
    "    region_folder = str(data_folder + '/' + stain1 + '/')\n",
    "    if region_code == '1':\n",
    "        shutil.move(str(region_folder + stain1_images), str(region_folder + region_list[0] + '/' + stain1_images))\n",
    "    elif region_code == '2':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[1] + '/' + stain1_images))\n",
    "    elif region_code == '3':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[2] + '/' + stain1_images))\n",
    "    elif region_code == '5':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[3] + '/' + stain1_images))\n",
    "    elif region_code == '6':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[4] + '/' + stain1_images))\n",
    "    elif region_code == '7':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[5] + '/' + stain1_images))\n",
    "    else:\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for stain1_images in file_list_stain1:\n",
    "    region_code = str(stain1_images[11])\n",
    "    region_folder = str(data_folder + '/' + stain1 + '/')\n",
    "    if region_code == '1':\n",
    "        shutil.move(str(region_folder + stain1_images), str(region_folder + region_list[0] + '/' + stain1_images))\n",
    "    elif region_code == '2':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[1] + '/' + stain1_images))\n",
    "    elif region_code == '3':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[2] + '/' + stain1_images))\n",
    "    elif region_code == '5':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[3] + '/' + stain1_images))\n",
    "    elif region_code == '6':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[4] + '/' + stain1_images))\n",
    "    elif region_code == '7':\n",
    "        shutil.move(str(region_folder + stain1_images),str(region_folder + region_list[5] + '/' + stain1_images))\n",
    "    else:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Folder breadown should be (1) stains, (2) animals, (3) regions\n",
    "#this code gets a list of the individual paths for each region folder with images\n",
    "threshold_paths = []\n",
    "for animals in folder_list:\n",
    "    region_list = os.listdir(str(data_folder + '/' + animals))\n",
    "    region_list = np.asarray(region_list)\n",
    "    \n",
    "    k=0\n",
    "    for regions in region_list:\n",
    "        if mac_annoyance in str(regions):\n",
    "            region_list = np.delete(region_list, (k), axis=0)\n",
    "        else:\n",
    "            k+=1\n",
    "    \n",
    "    for regions in region_list:\n",
    "        path = str(data_folder + '/' + animals + '/' + regions)\n",
    "        threshold_paths.append(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 3: Threshold and Segment Images*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3a: Import and threshold images from 'data_folder' from downloaded NDP images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*need to go in and add code to create a reasonable cut off for the min_size remove small objects*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Go in and add code that does a better job at separating overlapping cells (maybe this is just in thresholding*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paths in threshold_paths:\n",
    "\n",
    "    #Creating a folder for the new tresholded images\n",
    "    os.mkdir(str(paths + '/' + 'threshold_applied_images'))\n",
    "\n",
    "    #initializing a pandas dataframe\n",
    "    overall_region_meas = pd.DataFrame(columns = ['Filename', 'ImageID', 'ObjectID', 'X', 'Y', 'Area', 'Perimeter', 'Major Axis', 'Minor Axis', 'Circularity', 'Aspect Ratio'])\n",
    "\n",
    "    #initializing a count\n",
    "    \n",
    "    image_list = os.listdir(str(paths))\n",
    "    image_list = np.asarray(image_list)\n",
    "    \n",
    "    k=0\n",
    "    for images in image_list:\n",
    "        if image_file_type in str(images):\n",
    "            k+=1\n",
    "        else:\n",
    "            image_list = np.delete(image_list, (k), axis=0)\n",
    "    \n",
    "    \n",
    "    #Initializing an imag count\n",
    "    k=1\n",
    "    \n",
    "    #Looping through all images in list (this task should be put to multiple CPUs in future if a large data set)\n",
    "    for images in image_list:\n",
    "\n",
    "        #initializing a pandas data frame for measurement data\n",
    "        region_meas = pd.DataFrame()\n",
    "\n",
    "        #Going through each of the images to get their binarized images and measurement info\n",
    "        name = str(paths + '/' + images)\n",
    "        im = io.imread(name)\n",
    "        im = im[:,:,0]\n",
    "        threshold = filters.threshold_otsu(im)\n",
    "        binary = morphology.closing(im < threshold, morphology.square(5))\n",
    "        label_image = measure.label(binary, return_num=False, connectivity=2)\n",
    "        binary2 = morphology.remove_small_objects(label_image, min_size=400, connectivity=2, in_place=True)\n",
    "        invert_binary2 = np.invert(binary2)\n",
    "\n",
    "        # Saving the thresheld images to their own folder with modified names\n",
    "        im_to_save = Image.fromarray(np.uint8(binary2), mode='L')\n",
    "        new_name = str(paths + '/' + 'threshold_applied_images' + '/' + images)\n",
    "        new_name = new_name.replace('.jpg','_threshold_ch1.jpg')\n",
    "        im_to_save.save(new_name)\n",
    "\n",
    "\n",
    "        #Saving the region properties as a csv\n",
    "        props = measure.regionprops_table(binary2, properties= ('centroid', 'area', 'perimeter', 'major_axis_length', 'minor_axis_length'))\n",
    "        props_meas = pd.DataFrame(props)\n",
    "        region_meas['X'] = props_meas['centroid-0']\n",
    "        region_meas['Y'] = props_meas['centroid-1']\n",
    "        region_meas['Area'] = props_meas['area']\n",
    "        region_meas['Perimeter'] = props_meas['perimeter']\n",
    "        region_meas['Major Axis'] = props_meas['major_axis_length']\n",
    "        region_meas['Minor Axis'] = props_meas['minor_axis_length']\n",
    "        region_meas['Circularity'] = (region_meas['Area']*4*np.pi)/(region_meas['Perimeter']**2)\n",
    "        region_meas['Aspect Ratio'] = (region_meas['Major Axis']/region_meas['Minor Axis'])\n",
    "        region_meas.insert(0, 'Filename', images)\n",
    "        region_meas.insert(1, 'ImageID', k)\n",
    "        region_meas.insert(2, 'ObjectID', np.arange(len(region_meas)))\n",
    "\n",
    "        overall_region_meas = overall_region_meas.append(region_meas)\n",
    "        k+=1\n",
    "\n",
    "    path = str(paths + '/' + 'threshold_applied_images/')\n",
    "    overall_region_meas.to_csv(path + 'c1_registry.csv', index=False)\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Test and Train Split before Thresholding*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Go through all the threshold paths + the threshold folder and assign 8 of them for training and 2 of them for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = []\n",
    "for animals in folder_list:\n",
    "    region_list = os.listdir(str(data_folder + '/' + animals))\n",
    "    region_list = np.asarray(region_list)\n",
    "    \n",
    "    k=0\n",
    "    for regions in region_list:\n",
    "        if mac_annoyance in str(regions):\n",
    "            region_list = np.delete(region_list, (k), axis=0)\n",
    "        else:\n",
    "            k+=1\n",
    "    \n",
    "    for regions in region_list:\n",
    "        path = str(data_folder + '/' + animals + '/' + regions + '/' + 'threshold_applied_images/')\n",
    "        test_paths.append(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(str(data_folder + '/' + 'train'))\n",
    "\n",
    "name_count = 0\n",
    "\n",
    "for paths in test_paths:\n",
    "    \n",
    "    image_list = os.listdir(str(paths))\n",
    "    image_list = np.asarray(image_list)\n",
    "    \n",
    "    k=0\n",
    "    for images in image_list:\n",
    "        if image_file_type in str(images):\n",
    "            k+=1\n",
    "        else:\n",
    "            image_list = np.delete(image_list, (k), axis=0)\n",
    "\n",
    "    X_train, X_test= train_test_split(image_list, test_size=0.20, random_state=1)\n",
    "    \n",
    "    test_name_count = 1\n",
    "\n",
    "    for names in image_list:\n",
    "        \n",
    "        if names in X_train[:]:\n",
    "            shutil.move(str(paths + '/' + names), str(data_folder + '/' + 'train'))\n",
    "            \n",
    "            \n",
    "            train_name = str(data_folder + '/' + 'train/' + names)\n",
    "            string_count = str(name_count)\n",
    "            replace_name = str('xy' + string_count + stain + 'ch1' + '.jpg')\n",
    "            os.rename(str(data_folder + '/' + 'train/' + names), str(data_folder + '/' + 'train/' + replace_name))\n",
    "            \n",
    "            \n",
    "            name_count +=1\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            #renaming the testing groups\n",
    "            string_test_name_count = str(test_name_count)\n",
    "            os.rename(str(paths + '/' + names), str(paths + '/' + 'xy' + string_test_name_count + 'ch1' + '.jpg'))\n",
    "            \n",
    "            test_name_count += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Vampire Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The following will pull up the GUI as a popup - I want to be able to input values into the GUI straight from here not have to point and click*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#We are going to work with what we have an have a print out here of what to input in the VAMPIRE GUI - it is still faster than it used to be. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Build and then apply model with the vampire GUI (will work on making it not GUI later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Create the CSVs for Building and applying the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model repository to store the output information\n",
    "#create a csv for building the model\n",
    "#create a csv for applying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the directory in your data folder to put all information related to the model\n",
    "os.mkdir(str(data_folder + '/' + 'model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the csv for building a model\n",
    "data = [['1', stain, str(data_folder + '/' + 'train'), 'ch1', 'iba1']]\n",
    "build_model_csv = pd.DataFrame(data, columns = ['set ID', 'condition', 'set location', 'tag', 'note']) \n",
    "\n",
    "#saves csv to newly created model directory\n",
    "build_model_csv.to_csv(data_folder + '/' + 'model/' + 'images_to_build_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates the csv for applying a model\n",
    "apply_model_csv = pd.DataFrame(columns = ['set ID', 'condition', 'set location', 'tag', 'note'])\n",
    "\n",
    "set_number = 1\n",
    "for folders in test_paths:\n",
    "    str_set_number = str(set_number)\n",
    "    pattern = str(data_folder + '/' + '(.*?)' + '/threshold_applied_images/')\n",
    "    substring = re.search(pattern, folders).group(1)\n",
    "\n",
    "    df2 = pd.DataFrame({'set ID': [set_number], 'condition': [stain], 'set location': [folders], 'tag': ['ch1'], 'note': ['iba1']})\n",
    "    apply_model_csv = apply_model_csv.append(df2)\n",
    "    \n",
    "    set_number += 1\n",
    "    \n",
    "apply_model_csv.to_csv(data_folder + '/' + 'model/' + 'images_to_apply_model.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step X: Print the inputs that should be added to VAMPIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build Model CSV Path: /Users/hhelmbre/Desktop/6-17-2020-fiberfimages/iba1/model/images_to_build_model.csv\n",
      "Number of Shape Models (Recommended): 5\n",
      "Number of Shape Coordinates (Recommended): 50\n",
      "Model Name: practice\n",
      "Model to Apply: /Users/hhelmbre/Desktop/6-17-2020-fiberfimages/iba1/model/practice\n"
     ]
    }
   ],
   "source": [
    "print('Build Model CSV Path:', str(data_folder + '/' + 'model/' + 'images_to_build_model.csv'))\n",
    "\n",
    "print('Number of Shape Models (Recommended):', '5')\n",
    "print('Number of Shape Coordinates (Recommended):', '50')\n",
    "print('Model Name:', experiment_name)\n",
    "\n",
    "print('Apply Model CSV Path:', str(data_folder + '/' + 'model/' + 'images_to_apply_model.csv'))\n",
    "print('Model to Apply:', str(data_folder + '/' + 'model/' + experiment_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step X: Open the VAMPIRE GUI, build, and then apply the model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vampireanalysis\n",
    "from vampireanalysis import vampire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## getboundary.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 2897, in get_loc\n",
      "    return self._engine.get_loc(key)\n",
      "  File \"pandas/_libs/index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'ch1'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/tkinter/__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/vampireanalysis/vampire.py\", line 124, in <lambda>\n",
      "    b2 = Button(rows[5], text='build model', width=12, command=(lambda e=ents: Model(e, True, progress_bar)))\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/vampireanalysis/vampire.py\", line 83, in Model\n",
      "    getboundary(csv, progress_bar, entries)  # create registry csv and boundary stack\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/vampireanalysis/getboundary.py\", line 51, in getboundary\n",
      "    ch1 = ui['ch1'][setfolderidx]\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/pandas/core/frame.py\", line 2995, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"/Users/hhelmbre/opt/anaconda3/envs/vampirenv/lib/python3.7/site-packages/pandas/core/indexes/base.py\", line 2899, in get_loc\n",
      "    return self._engine.get_loc(self._maybe_cast_indexer(key))\n",
      "  File \"pandas/_libs/index.pyx\", line 107, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/index.pyx\", line 131, in pandas._libs.index.IndexEngine.get_loc\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1607, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "  File \"pandas/_libs/hashtable_class_helper.pxi\", line 1614, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "KeyError: 'ch1'\n"
     ]
    }
   ],
   "source": [
    "vampire()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 8: New visualization of VAMPIRE data*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This should probably be built into VAMPIRE and not just into this notebook - think about this for the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still needs to be visualization steps here because what is output by vampire just is not great"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
